from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, roc_auc_score,roc_curve
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import BernoulliNB,GaussianNB ,MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn import svm
import pandas as pd
import lightgbm as lgbm
import numpy as np
import pydotplus
import matplotlib.pyplot as plt


subtrainLabel = pd.read_csv('.\\subtrainLabels.csv')
subtrainfeature = pd.read_csv(".\\3gramfeature.csv")
subtrain = pd.merge(subtrainLabel,subtrainfeature,on='Id')
labels = subtrain.Class
subtrain.drop(["Class","Id"], axis=1, inplace=True)
subtrain = subtrain.values

x_train, x_test, y_train, y_test = train_test_split(subtrain,labels,test_size=0.4)

print('DecisionTree-ngram训练测试子集--------')

DT = tree.DecisionTreeClassifier()
DT.fit(x_train, y_train)
y_predict = DT.predict(x_test)

fpr, tpr, thresholds = roc_curve(y_test, y_predict, pos_label=1)
accuracy_DT = accuracy_score(y_test, y_predict)
roc_auc_score_DT = roc_auc_score(y_test, y_predict)
precision_DT = precision_score(y_test, y_predict, average='weighted')
recall_DT = recall_score(y_test, y_predict, average='weighted')
f1_DT = f1_score(y_test, y_predict, average='weighted')
print('roc_auc_score:' , roc_auc_score_DT)
print('accuracy:', accuracy_DT)
print('precision:', precision_DT)
print('recall:', recall_DT)
print('f1-score:', f1_DT)
plt.plot(fpr, tpr, color='purple', label='DT')



print('LinearSVC-ngram训练测试子集------------')

svm = svm.SVC(kernel='linear', C=1000)
svm.fit(x_train, y_train)
y_predict = svm.predict(x_test)

fpr, tpr, thresholds = roc_curve(y_test, y_predict, pos_label=1)
accuracy_svm = accuracy_score(y_test, y_predict)
roc_auc_score_svm = roc_auc_score(y_test, y_predict)
precision_svm = precision_score(y_test, y_predict, average='weighted')
recall_svm = recall_score(y_test, y_predict, average='weighted')
f1_svm = f1_score(y_test, y_predict, average='weighted')
print('roc_auc_score:' , roc_auc_score(y_test, y_predict))
print('accuracy:', accuracy_svm)
print('precision:', precision_svm)
print('recall:', recall_svm)
print('f1-score:', f1_svm)
plt.plot(fpr, tpr, color='blue', label='SVM')

print('KNeighbors-ngram训练测试子集-----------')

knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
y_predict = knn.predict(x_test)

fpr, tpr, thresholds = roc_curve(y_test, y_predict, pos_label=1)
accuracy_knn = accuracy_score(y_test, y_predict)
roc_auc_score_knn = roc_auc_score(y_test, y_predict)
precision_knn = precision_score(y_test, y_predict, average='weighted')
recall_knn = recall_score(y_test, y_predict, average='weighted')
f1_knn = f1_score(y_test, y_predict, average='weighted')
print('roc_auc_score:' , roc_auc_score_knn)
print('accuracy:', accuracy_knn)
print('precision:', precision_knn)
print('recall:', recall_knn)
print('f1-score:', f1_knn)
plt.plot(fpr, tpr, color='skyblue', label='KNN')

print('GaussianNB-ngram训练测试子集--------')

bayes = GaussianNB()
bayes.fit(x_train, y_train)
y_predict = bayes.predict(x_test)

fpr, tpr, thresholds = roc_curve(y_test, y_predict, pos_label=1)
accuracy_bayes = accuracy_score(y_test, y_predict)
roc_auc_score_bayes = roc_auc_score(y_test, y_predict)
precision_bayes = precision_score(y_test, y_predict, average='weighted')
recall_bayes = recall_score(y_test, y_predict, average='weighted')
f1_bayes = f1_score(y_test, y_predict, average='weighted')
print('roc_auc_score:' , roc_auc_score_bayes)
print('accuracy:', accuracy_bayes)
print('precision:', precision_bayes)
print('recall:', recall_bayes)
print('f1-score:', f1_bayes)
plt.plot(fpr, tpr, color='green', label='GNB')

print('LRegression-ngram训练测试子集-------')

LR = LogisticRegression()
LR.fit(x_train, y_train)
y_predict = LR.predict(x_test)

fpr, tpr, thresholds = roc_curve(y_test, y_predict, pos_label=1)
accuracy_LR = accuracy_score(y_test, y_predict)
roc_auc_score_LR = roc_auc_score(y_test, y_predict)
precision_LR = precision_score(y_test, y_predict, average='weighted')
recall_LR = recall_score(y_test, y_predict, average='weighted')
f1_LR = f1_score(y_test, y_predict, average='weighted')
print('roc_auc_score:' , roc_auc_score(y_test, y_predict))
print('accuracy:', accuracy_LR)
print('precision:', precision_LR)
print('recall:', recall_LR)
print('f1-score:', f1_LR)
plt.plot(fpr, tpr, color='orchid', label='LR')

print('RandomForest-ngram训练测试子集-------')

rf = RF()
rf.fit(x_train,y_train)
y_predict = rf.predict(x_test)
fpr, tpr, thresholds = roc_curve(y_test, y_predict, pos_label=1)
accuracy_rf = accuracy_score(y_test, y_predict)
roc_auc_score_rf = roc_auc_score(y_test, y_predict)
precision_rf = precision_score(y_test, y_predict, average='weighted')
recall_rf = recall_score(y_test, y_predict, average='weighted')
f1_rf = f1_score(y_test, y_predict, average='weighted')
print('roc_auc_score:' , roc_auc_score(y_test, y_predict))
print('accuracy:', accuracy_rf)
print('precision:', precision_rf)
print('recall:', recall_rf)
print('f1-score:', f1_rf)
plt.plot(fpr, tpr, color='orange', label='RF')

print('LightGBM-ngram训练测试子集------------')

lgb = lgbm.LGBMClassifier()
lgb.fit(x_train,y_train)
y_predict = lgb.predict(x_test)
accuracy_lgb = accuracy_score(y_test, y_predict)
roc_auc_score_lgb = roc_auc_score(y_test, y_predict)
fpr, tpr, thresholds = roc_curve(y_test, y_predict, pos_label=1)
precision_lgb = precision_score(y_test, y_predict, average='weighted')
recall_lgb = recall_score(y_test, y_predict, average='weighted')
f1_lgb = f1_score(y_test, y_predict, average='weighted')
print('roc_auc_score:' , roc_auc_score(y_test, y_predict))
print('accuracy:', accuracy_lgb)
print('precision:', precision_lgb)
print('recall:', recall_lgb)
print('f1-score:', f1_lgb)
plt.plot(fpr, tpr, color='red', label='LGBM')

plt.axhline(0.5, 0.5,color='black', linestyle='--')

# ax = lgbm.plot_tree(lgb, tree_index=1, figsize=(20, 8), show_info=['split_gain'])
# plt.show()
#
# graph = lgbm.create_tree_digraph(lgb, tree_index=10, name='Tree')
# graph.render(view=True)
#
# ax = lgbm.plot_importance(lgb, max_num_features=10)#max_features表示最多展示出前10个重要性特征，可以自行设置
# plt.show()

plt.legend()
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.show()